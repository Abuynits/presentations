\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{nicefrac}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{bm}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage[export]{adjustbox}
\usepackage{svg}

\usetheme{Madrid}
\definecolor{mlpblue}{rgb}{0.1, 0.14, 0.24}

\useoutertheme{infolines} % Alternatively: miniframes, infolines, split
\useinnertheme{circles}
\usecolortheme[named=mlpblue]{structure}

\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Concat}{Concat}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\indep}{\perp \!\!\! \perp}

\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}

%------------------------------------------------------------
%This block of code defines the information to appear in the
%Title page
\title[Cosmos ]{World Foundation Model Platform for Physical AI}

\subtitle{Cosmos} %\thanks{Beaglehole, Súkeník et.~al.~[NeurIPS 2024]}}
\author[MLP]{A. Buynitsky} 
\date{Jan 28, 2025}
\titlegraphic{\includegraphics[width=8cm]{./img/title.png}}

%End of title page configuration block
%------------------------------------------------------------

%The next block of commands puts the table of contents at the 
%beginning of each section and highlights the current section:

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Outline}
    \tableofcontents[currentsection]
  \end{frame}
}
% ------------------------------------------------------------


\begin{document}

\frame{\titlepage}


%---------------------------------------------------------
% This block of code is for the table of contents after
% the title page
\begin{frame}
\frametitle{Outline}
\tableofcontents
\end{frame}
%---------------------------------------------------------
\section{TLDR}
\begin{frame}[t]{TLDR (part 1)}
    \textbf{Problem:} More efficient to train agents virtually which requires virtual platforms \newline
    \pause
    \textbf{Solution:} World Foundation Models (WFMs) that can be pretrained and finetuned similar to LLMs\newline
    \begin{center}
        \includegraphics[width=0.8\textwidth]{./img/tldr_2.png}
    \end{center} \pause
    \textbf{WFM Goal}: predict future observation $\hat{x}_{t+1}$ based on $ x_{0:t}$ and $c_t$
    \small
    \begin{itemize}[label=-]
        \item $ x_{0:t}$ sequence of $t$ timesteps of visual observed data
        \item $c_t$ takes on many forms (actions, trajectories, text descriptions)
        \item $ x_{0:t}$ sequence of $t$ timesteps of visual observed data
    \end{itemize}
    \normalsize
            
\end{frame}

\begin{frame}[t]{TLDR (part 2)}
    \textbf{Problem:} More efficient to train agents virtually which requires virtual platforms \newline
    \textbf{Solution:} World Foundation Models (WFMs) that can be pretrained and finetuned similar to LLMs\newline
    \textbf{Results:} 
    \begin{center}
        \includegraphics[width=\textwidth]{./img/tldr_1.png}
    \end{center} 
    \textbf{Research Group:} NVIDIA + 1X\newline \pause
    \textbf{Future Work:}
    \small
    \begin{itemize}[label=-]
        \item \textbf{Policy Evaluation:} benchmarking policies in simulator vs real-world
        \item \textbf{Policy Initialization:} well-trained WFM can initialize policies 
        \item \textbf{Policy Training:} WFM pairmed with reward can give feedback in RL
        \item \textbf{MPC Control:} WFM predict future sttates of physical system
    \end{itemize}
    \normalsize
\end{frame}


\begin{frame}[t]{TLDR (part 3)}
    \begin{center}
        \includegraphics[width=\textwidth]{./img/tldr_3.png}
    \end{center} 
    \small
    \begin{itemize}[label=-]
        \item \textbf{Video Currator:} Collecting high-quality pre-training data
        \item \textbf{Tokenizers:} Image and video tokenizers
        \item \textbf{Pre-trained WFMs:} Diffusion and Autoregressive models
        % text2world generation pre-training
        % video2world generation pre-training
        \item \textbf{WFMs Post-Training:} finetuning for specific robotic tasks
        \item \textbf{Guardrail:} safety to block harmful input and outputs
    \end{itemize}
    \normalsize
\end{frame}


\section{Data Curation}

\begin{frame}[t]{Raw Data}
    Collect 20M hours of videos from a diverse set of physical AI environments
    \begin{columns}
        \hspace{1em}
		\begin{column}{.5\textwidth}
            \small
            \begin{enumerate}[label=\arabic*., itemsep=0.5mm]
                \item Driving (11\%)
                \item Hand motion and object manipulation (16\%)
                \item Human motion and activity (10\%)
                \item Spatial awareness and navigation (16\%)
                \item First person point-of-view (8\%)
                \item Nature dynamics (20\%)
                \item Dynamic camera movements (8\%)
                \item Synthetically rendered (4\%)
                \item Others (7\%)
            \end{enumerate}
            \normalsize
		\end{column}
        \hspace{0em}
		\begin{column}{.5\textwidth}
            \pause
            \textbf{Problem:} redundant / useless information \newline
            \textbf{Solution:} Filtering / Postprocessing \newline
            \textbf{Result:} $10^8$ pre-training video clips and $10^7$ finetuning clips 

		\end{column}
        \hspace{2em}
	\end{columns}
\end{frame}

\begin{frame}[t]{Splitting}
    \textbf{Problem:} Videos have arbitrary lenghts and might contain shot transitions
    % ex: two ppl talking in kitchen in NYC to lions in savana
    % important to segment videos base on shot chagnes and generate consistent video clips physically plausible
    \textbf{Shot Detection}
    \begin{itemize}[label=-]
        \item Generate start and end frame indices of clips using TransNetV2
        \item Discard clips shorter than 2s, slipt clips over 60s long % (visual effects / transitions)
        % create own internal benchmark, TransNetV2
        % Normalize all videos to the same codec 
    \end{itemize}
    \pause
    \textbf{Filtering} % remove clips with bad quality, select high-quality for fine-tuning, tailor data distrubtion for MFs
    \begin{itemize}[label=-]
        \item \textbf{Motion Filtering:} remove static videos / with abrupt camera motion / tag videos with differnet camera motion 
        % create own internal benchmark, TransNetV2
    \end{itemize}
\end{frame}


\section{Tokenizer}
\begin{frame}[t]{Tokenizers}
    Visual tokenizers map raw visual data (images, videos) to compact tokens
        \begin{center}
            \includegraphics[width=0.8\textwidth]{./img/tokenizer_1.png}
        \end{center}
	\begin{columns}[t]
		\begin{column}{.5\textwidth}
            \textbf{Continuous Tokenizers:} Encode into continuous latent embeddings \newline
            % cont tokenizers: a set of vectors as in stable diffusion
            \vspace{-1.5em}
			\begin{center}
                \includegraphics[width=0.5\textwidth]{./img/tokenizer_cont.png}
			\end{center}
		\end{column}
		\begin{column}{.5\textwidth}
            \textbf{Discrete Tokenizers:} Encode into discrete latent codes
            % disc tokenizers: make to quantized indices ie autoregressive transformers GPT
            % trade off between high compression rates without compromizing subsequent visual reconstruction quality
            % high comperssing reduces storage / computation
            % excessvie compression lead to loss of details
            \vspace{-0.5em}
			\begin{center}
                \includegraphics[width=0.5\textwidth]{./img/tokenizer_discrete.png}
			\end{center}
		\end{column}
	\end{columns}
    % 1 + just to not have 0
    $S_{HW}$: Spatial compression factor, $S_T$: Temporal compression factor
\end{frame}

\begin{frame}[t]{Cosmos Tokenizer}
    Encoder Decoder Architecture: given $x_{0:T} \in R^{(1+T) \times H \times W \times 3}$
        \begin{gather}\label{eq:1}
            \varepsilon(x_{0:T}) = z_{0:T'} \in \mathbb{R}^{(1+T') \times H' \times W' \times C'}  \\
            \mathcal{D}(z_{0:T'}) = \hat{x}_{0:T} \in R^{(1+T) \times H \times W \times 3}
        \end{gather}
	\begin{columns}[t]
		\begin{column}{.5\textwidth}
            \textbf{Continuous Tokenizers:} Encode into continuous latent embeddings \newline
            % cont tokenizers: a set of vectors as in stable diffusion
            \vspace{-1.5em}
			\begin{center}
                \includegraphics[width=0.5\textwidth]{./img/tokenizer_cont.png}
			\end{center}
		\end{column}
		\begin{column}{.5\textwidth}
            \textbf{Discrete Tokenizers:} Encode into discrete latent codes
            % disc tokenizers: make to quantized indices ie autoregressive transformers GPT
            % trade off between high compression rates without compromizing subsequent visual reconstruction quality
            % high comperssing reduces storage / computation
            % excessvie compression lead to loss of details
            \vspace{-0.5em}
			\begin{center}
                \includegraphics[width=0.5\textwidth]{./img/tokenizer_discrete.png}
			\end{center}
		\end{column}
	\end{columns}
    % 1 + just to not have 0
    $S_{HW}$: Spatial compression factor, $S_T$: Temporal compression factor
\end{frame}

\begin{frame}[t]{Cosmos Tokenizer}
    \vspace{-2em}
	\begin{columns}[t]
		\begin{column}{.5\textwidth}
            \begin{center}
                \includegraphics[width=0.8\textwidth]{./img/tokenizer_2.png}
            \end{center}
		\end{column}
		\begin{column}{.5\textwidth}
            \begin{center}
                \includegraphics[width=0.8\textwidth]{./img/tokenizer_3.png}
            \end{center}
		\end{column}
	\end{columns}
    \small
\end{frame}

\begin{frame}[t]{Wavelet Transform}
    \textbf{Wavelet:} compress long signals into smaller ones
    \[
    \text{Let } W = 
    \left
    \{
    \begin{vmatrix}
      1 \\
      1 \\
      1 \\
      1 
    \end{vmatrix}
    ,
    \begin{vmatrix}
      1 \\
      1 \\
     -1 \\
     -1 
    \end{vmatrix}
    ,
    \begin{vmatrix}
      1 \\
     -1 \\
      0 \\
      0
    \end{vmatrix}
    ,
    \begin{vmatrix}
      0 \\
      0 \\
      1 \\
     -1
    \end{vmatrix}
    \right
    \}
    \text{be the Haar basis}
    \]
    \[
    E = \left\{e_1, e_2, e_3, e_4 \right\} \text{be standard basis of }  \mathbb{R}^4
    \]
    \normalsize
    (pairwise orthogonal vectors are linearly independent)\newline
    \pause
    The change of basis matrix from $W$ to $E$, $T_{W,E}$ is given by:
    \[T_{W,E} = 
        \begin{vmatrix}
            1 & 1 & 1 & 0 \\
            1 & 1 & -1 & 0\\
            1 & -1 & 0 & 1\\
            1 & -1 & 0 & -1 
          \end{vmatrix}
    \]
\end{frame} 


\begin{frame}[t]{Wavelet Transform}
    Given a signal $v =\begin{vmatrix} v_1 & v_2 & v_3 & v_4 \end{vmatrix}^T$, transform 
    $v$ into its coefficients over the Haar Basis by computing $c=\begin{vmatrix} c_1 & c_2 & c_3 & c_4 \end{vmatrix}^T= T_{W,E}^{-1} v$ \newline

    \textbf{Example:}
    Take $v = \begin{vmatrix} 6 & 4 & 5 & 1 \end{vmatrix} $ over the basis $E$
    \[c= T_{W,E}^{-1} \cdot v = \begin{vmatrix} 4 & 1 & 1 & 2 \end{vmatrix}\] 
    Observe that: $c_1 =\frac{v_1 + v_2 + v_3 + v_4}{4}$ is the average value of signal $v$ %correspond to background of the image.
    \newline
    $c_2$ gives coarse details of $v$ \newline 
    $c_3$ gives details in first part of $v$ \newline
    $c_4$ gives details in second part of $v$ \newline
    Reconstruct signal by $v = T_{W,E} \cdot c$ \newline
    \newline
    \textbf{Compression:} Throw away some of coefficients of $c$ (settinig them to zero)
    \newline
    Cosmos Tokenizer does two stages of compression, reducing resultion of $x$, $y$ and $t$ by half in each (4x reduction total)\newline
\end{frame} 

\begin{frame}[t]{Cosmos Tokenizer}
    \vspace{-2em}
	\begin{columns}[t]
		\begin{column}{.5\textwidth}
            \begin{center}
                \includegraphics[width=0.8\textwidth]{./img/tokenizer_2.png}
            \end{center}
		\end{column}
		\begin{column}{.5\textwidth}
            \begin{center}
                \includegraphics[width=0.8\textwidth]{./img/tokenizer_3.png}
            \end{center}
		\end{column}
	\end{columns}
    \textbf{Downsampling}
    \begin{itemize}[label=-]
        \item 2D convolution with kernel $1 \times k \times k$ for spatial info 
        \item 1D convolution with kernel $k \times 1 \times 1$ for temporal info (left padded with $k-1$)
        \item spatio-temporal attention (ie. $1 + T'$ for last encoder block)
        % spatio-temoral attention: at time i, only attend from 0:i
    \end{itemize}
    \textbf{Continuous:} Use vanilla Autoencoder for latent space (dim = 16)
    \textbf{Discrete:} project using Finite-Scalar-Quantization to 64k vocab size
    % FSQ: small dimension (they used 6), and each dimension is quantized to small set of fixed values
\end{frame}

\begin{frame}[t]{Cosmos Tokenizer Training}
    Supervise the final output of tokenizer's decoder (no aux loss ie KL), train by alternating images and videos \newline
    \textbf{Stage 1:}
    \[\mathcal{L}_1 = \| \hat{x}_{0:T} - x_{0:T} \|_1\]
    % xhat is reconstructed image
    \[\mathcal{L}_{\text{Perceptual}} = \frac{1}{L} \sum_{l=1}^{L} \sum_{t} \alpha_l \| \text{VGG}_l(\hat{x}_t) - \text{VGG}_l(x_t) \|_1\]
    % vgg  alpha: predefined weights
    \textbf{Stage 2:} \newline
    % optical flow: how objects movethrough video 
    \textbf{Optical Flow loss} for temporal smoothness
    \[\mathcal{L}_{\text{Flow}} = \frac{1}{T} \sum_{t=1}^{T} \| \text{OF}(\hat{x}_t, \hat{x}_{t-1}) - \text{OF}(x_t, x_{t-1}) \|_1\]    
    \textbf{Gram-matrix loss} for sharpness of reconstructed images
    \[\mathcal{L}_{\text{Gram}} = \frac{1}{L} \sum_{l=1}^{L} \sum_{t} \alpha_l \| \text{GM}_l(\hat{x}_t) - \text{GM}_l(x_t) \|_1\]
\end{frame}
% results: faster / better than other tokenizers for video data, 2x -12x faster
% have a spatila compression of 4 x 8 x 8 is best, even lower, 8 x 8 x 4 or 8 x 16 x16, have comparable perf
\section{Diffusion-based WFM}

\begin{frame}[t]{Formulation}
    \textbf{Denoising score mathcing loss} for denoisier $D_\theta$ evaluated at noise $\sigma$ where $x_o \sim p_\text{data}$
    and $n \sim \mathcal{N}(0, \sigma^2 \mathbf{I})$ 
    \begin{gather}\label{eq:2}
        \mathcal{L}(D_\theta, \sigma) = \mathbb{E}_{x_0, n} \left[ \left\| D_\theta(x_0 + n; \sigma) - x_0 \right\|_2^2 \right]
        % x_o is clear image or video from training set, n is gauissian noise 
        % D_o is noise condiened nn to denoise corrpited sample
    \end{gather}
    \textbf{Overall training loss} as weighted expectation over all noise levels:
    \begin{gather}\label{eq:3}
        \mathcal{L}(D_\theta) = \mathbb{E}_\sigma \left[ \frac{\lambda(\sigma)}{e^{u(\sigma)}} \mathcal{L}(D_\theta, \sigma)+ u(\sigma) \right] \\
        % \lambda(\sigma) = \frac{\sigma^2 + \sigma_\text{data}^2}{(\sigma \cdot \sigma_\text{data})^2}\\
        % \ln(\sigma) \sim \mathcal{N}(P_\text{mean}, P_\text{std}^2)
    \end{gather}
    % $P_\text{mean}$ and $P_\text{std}$ are hyperparamters and $\sigma_\text{data}$ is the data mean\newline

    % problem: as training progresses, the balance may deteriorate
    % use uncertainty-based weighting which quantifies the uncertaining for the denoising objective at \sigma
    $\lambda(\sigma)$: Weighting function \newline 
    $u(\sigma)$ MLP that quantifies uncertainty of task.\newline
    $u(\sigma)$ \textbf{weights down} loss contribution if uncertain about task ($u(\sigma)$ high)
    $u(\sigma)$ \textbf{weights up} loss contribution if certain about task ($u(\sigma)$ low)
    \newline
    Model penalazied for uncertainty (want $u(\sigma)$ to be low)
\end{frame}

\begin{frame}[t]{Architecture (input \& 3D Patchify)}
    \vspace{-1.6em}
    \begin{center}
        \includegraphics[width=0.7\textwidth]{./img/diffusion_arch.png}
    \end{center}
    % have video, run in through tokenizer, pertubate with gaussian noise
    \textbf{3D Patchification:}
    \begin{itemize}[label=-]
        \item From tokenizer, input is latent representation $x \in R^{T \times C \times H \times W}$ 
        \item Downsample cubes of shape $p_t, p_h, p_w$ into individual input tokens via linear layer and flatten to 1D vector
        \item image / videos reshaped to 1D, spatio temporal seq length $\frac{T H W}{p_t p_h p_w}$
        \item use $p_t = 1, ,p_h=p_w=2$
    \end{itemize}
\end{frame}

\begin{frame}[t]{Architecture (positional encoding)}
    \vspace{-1.6em}
    \begin{center}
        \includegraphics[width=0.7\textwidth]{./img/diffusion_arch.png}
    \end{center}
    % have video, run in through tokenizer, pertubate with gaussian noise
    \textbf{Positional Encoding:}
    \newline
    I didnt want to hand wave it away soo... detour to ROPE!!!!
\end{frame}

\begin{frame}[t]{RoPE [Problem Formulation]}
    $\mathbb{S}_N = \left\{w_i \right\}_{i=1}^N$: $N$ input tokens, 
    % each token mapped to a embedidng (either learned during training or pretrained ie word2vec)
    $\mathbb{E}_N = \left\{x_i \right\}_{i=1}^N$ be word embeddings \newline
    with: $x_i \in \mathbb{R}^d$ embedding vector of token $w_i$ \textbf{without positional info}
    \newline
    \newline
    % encode two characteristics tokens:
    %1. tokens that are similar / related should have higher scores
    %2. score should be higher for words that are closer together
    self attn calculates: $q_m = f_q(x_m, m), k_n = f_k(x_n, n), v_n = f_v(x_n, n)$ with:
    \begin{gather}\label{eq:4}
        f_{t:t\in \left\{q,k,v\right\}} = \textbf{W}_{t:t\in \left\{q,k,v\right\}}(x_i + p_i)
    \end{gather}
    where $p_i$ is:
    \[
        \begin{cases}
        \begin{aligned}
        p_{2t} & = \sin\left(\frac{k}{10000} \cdot \frac{2t}{d}\right), \\
        p_{2t+1} & = \cos\left(\frac{k}{10000} \cdot \frac{2t}{d}\right).
        \end{aligned}
        \end{cases}
    \]
\end{frame}

\begin{frame}[t]{RoPE [Issue with positional encodings]}
    % tokens that have similar embeddigns should have higher score
    % toekns that are close to each other should have higher score
    Lets consider a 2D case:
    \begin{itemize}[label=-]
        % think of dot product as magnitude and angle
        % when multiple k,q matrices, each square shows the weight to place on the value between word at n, word at m
        \item Similar tokens should have higher score $||q_m|| \cdot ||k_n||$ (magnitude) 
        % further apart words are, less likely for them to be related
        \item Closer tokens should have higher score $\Theta_{(m-n)}$
    \end{itemize}
    \begin{center}
        \includegraphics[width=0.9\textwidth]{./img/rope_1.png}
    \end{center}
    \small Shoutout to https://www.youtube.com/watch?v=GQPOtyITy54 for visuals
    \normalsize
\end{frame}

\begin{frame}[t]{RoPE [Issue with positional encodings (cont)]}
    % tokens that have similar embeddigns should have higher score
    % toekns that are close to each other should have higher score
    Lets consider a 2D case:
    \begin{itemize}[label=-]
        % think of dot product as magnitude and angle
        % when multiple k,q matrices, each square shows the weight to place on the value between word at n, word at m
        \item Similar tokens should have higher score $||q_m|| \cdot ||k_n||$ (magnitude) 
        % further apart words are, less likely for them to be related
        \item Closer tokens should have higher score $\Theta_{(m-n)}$
    \end{itemize}
    \begin{center}
        \includegraphics[width=0.8\textwidth]{./img/rope_2.png}
    \end{center}
    Problem: Combined positional and token embeddings together
\end{frame}

\begin{frame}[t]{RoPE [Reformulation]}
    For each inner product, we want: \[q_m^T k_n = \langle f_q(x_m, m), f_k(x_n, n) \rangle = g(x_m, x_n, m - n)\]
    In 2D case, convert vectors to complex numbers:
    \begin{align}
        q_m &= R_q(x_m, m)e^{i \Theta_q(x_m,m)} \\
        k_n &= R_k(x_n, n)e^{i \Theta_k(x_n,n)} \\
        g(x_m, x_n, n - m) &= R_g(x_m, x_n, n - m) e^{i \Theta_g(x_m, x_n, n - m)}\\
        q_m^T &= R_q(x_m, m)e^{-i \Theta_q(x_m,m)}
    \end{align}
    Multiply $q_m^T \cdot k_n$ in complex numbers:
    \begin{align}
        q_m^T \cdot k_n = R_q(x_m, m) \cdot R_k(x_n, n) e^{i (\Theta_k(x_n,n) - \Theta_q(x_m,m))}
    \end{align}
    % solve and get the follwoing two equations:
    % m - n represents relative position info between two vectors
    % when take transpose, same as taking complex conjugate
\end{frame}

\begin{frame}[t]{RoPE [Determine Magintude]}
    \vspace{-3em}
    \begin{gather}\label{eq:mag}
        R_q(x_m, m) R_k(x_n, n) = R_g(x_m, x_n, n - m)
    \end{gather}
    \vspace{-3em}
    \begin{gather}\label{eq:arg}
        \Theta_k(x_n, n) - \Theta_q(x_m, m) = \Theta_g(x_m, x_n, n - m)
    \end{gather}
    % will mean that equ 12 will go to 0 
    Now suppose that $m = n$. Then left with just \ref{eq:mag} \newline
    \[R_q(x_m, m) R_k(x_n, n) = R_g(x_m, x_n, 0)\]
    Now set $m = n = 0$
    \[R_g(x_m, x_n, 0) = R_q(x_m, 0) R_k(x_n, 0) = ||q|| \; ||k ||\]
    Therefore:
    \[R_q(x_m, m) = ||q|| \text{ and } R_k(x_n, n) = || k ||\]
\end{frame}

\begin{frame}[t]{RoPE [Determine Argument]}
    By a similar trick setting $m=n$:
    \begin{align}
        \Theta_q(x_m, m) - \Theta(x_n, n) &= \Theta_g(x_m, x_n, 0)\\
        &= \Theta(x_m, 0) - \Theta(x_n, 0) = \theta_q - \theta_k
    \end{align}
    Rearranging, we get:
    \[\Theta_q(x_m, m) - \theta_q = \Theta_k(x_n, m) - \theta_k\]
    Observe that values only related to $m$, independent if $x = x_n$ or $x = x_m$ \newline
    Define $\phi(m)$ to be:
    % can take either side of the equation
    \[\phi(m) = \Theta_q(x_m, m) - \theta_q\]
\end{frame}

\begin{frame}[t]{RoPE [Determine Argument]}
    Now consider analyzing the difference between $\phi(m + 1)$ and $\phi(m)$:
    \begin{align}
        \phi(m + 1) - \phi(m) &= \Theta_q(x_m, m+1) - \theta_q - (\Theta_q(x_m, m) - \theta_q)\\
                              &= \Theta_q(x_m, m+1) - \Theta_q(x_m, m)\\
                              &= \Theta_q(x_m, m+1) - \Theta_q(x_m, m)\\
                              &= \Theta_g(x_m, x_m, 1)
    \end{align}
    $\phi(m)$ has constant difference between term regardless of $m$ so its an arithmetic sequence!\newline
    \[\phi(m) = m \theta + \gamma\]
\end{frame}

\begin{frame}[t]{RoPE [Combining Everything]}
    \vspace{-2.5em}
    \begin{align}
        % q is q_m 
        q_m = f_q(x_m, m) &= R_q(x_m, m) e^{i \Theta_q (x_m, m)}\\
                          &= ||q|| e^{i \Theta_q (x_m, m)}\\
                          &= ||q|| e^{i \phi(m) + \theta_q}\\
                          &= ||q|| e^{i m \theta + \gamma + \theta_q}\\
                          &= ||q|| e^{i \theta_q}e^{i (m \theta + \gamma)}\\
                          &= q e^{i (m \theta + \gamma)}
    \end{align}
    Recall that:
    \[q_m = f_q(x_m, m) = W_q(x_m + p_m)\]
    so if $p_m = 0$, then
    \[ q_m = W_q \cdot x_m\]
    Therefore assume no position info when $m=0$ so let $\gamma = 0$.\newline
    % do not rotate the input
    % cool thing get is roots of unity
    \textbf{Final Result:}
    \[ f_q(x_m, m) = (W_q x_m)e^{i m \theta}\]
    \[ f_k(x_n, n) = (W_k x_n)e^{i n \theta}\]

\end{frame}


\begin{frame}[t]{RoPE [Result]}
    Matrix form of equations in 2D
    \begin{center}
        \includegraphics[width=1.0\textwidth]{./img/rope_3.png}
    \end{center}
\end{frame}

\begin{frame}[t]{RoPE [Result]}
    Extending to multiple dimensions
    % break q and k d dimensional in d/2 blocks will repeat for each block indep
    % convert to block diag rotational matrix, each rotation block have own constant
    % will depend on index across dimension
    \begin{center}
        \includegraphics[width=1.0\textwidth]{./img/rope_4.png}
    \end{center}
\end{frame}

\begin{frame}[t]{RoPE [Result]}
    Increases Sequence prediction confidence (perplexity):
    % perplexity measures how confused model is (think predicting 5 words with = probability)
    \begin{center}
        \includegraphics[width=0.8\textwidth]{./img/rope_5.png}
    \end{center}
\end{frame}


\begin{frame}[t]{Architecture (positional encoding)}
    \vspace{-1.6em}
    \begin{center}
        \includegraphics[width=0.7\textwidth]{./img/diffusion_arch.png}
    \end{center}
    % have video, run in through tokenizer, pertubate with gaussian noise
    % partition feature dim into 3 approx equal chunks, apply RoPE along temporral, height, width axes
    \begin{itemize}[label=-]
        \item partition feature dim into 3 approx equal chunks representing $T, W, H$, apply RoPE across each dimension
        % apply to the corresponding patches between splitting 
        \item Rescale temporal frequencies based on video FPS
        \item Add an absolute (learnable) positional embedding % increases perfrormance
   \end{itemize}
\end{frame}


\begin{frame}[t]{Architecture (Attention)}
    \vspace{-1.6em}
    \begin{center}
        \includegraphics[width=0.7\textwidth]{./img/diffusion_arch.png}
    \end{center}
    \textbf{Cross Attention:} keys and querries from T5-XXL etext encoder\newline

    \textbf{Query-Key Normalization} before attention (RMSNorm) 
    % each transformer block is sequential self.attn, ccross-attn, and feed foward layers
    \begin{itemize}[label=-]
        \item  QK-norm applied before attn layers
        \item  fucus on re-scaling, ignore re-centering aspect of normalization
        \item Significantly reduce computation
        % apply RMSNorm normalization across hear of each query and key matrix priori to multiplying
        % only focus on re-scaling and ingoring the re-centering aspect or normalization
        % significantly reduct computation
        % then scale up by learnable parameter
   \end{itemize}
\end{frame}

\begin{frame}{Thank you!}
	\begin{center}
        Have a great rest of your Day!!!
	\end{center}
	\begin{center}
		% \textbf{Slides:} {\small \url{https://cs.purdue.edu/homes/jsetpal/slides/dnc_by_agop.pdf}}
	\end{center}
\end{frame}


\end{document}